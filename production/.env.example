# Production Test Framework Configuration
# Copy this file to `.env` and adjust values for your environment.

# =============================================================================
# WebSocket Configuration
# =============================================================================

# WebSocket endpoint for the translation service under test
TRANSLATION_WEBSOCKET_URL=ws://localhost:8000/ws

# Optional authentication key or token for the translation WebSocket
# Leave empty if not required
TRANSLATION_WS_AUTH=

# Connection timeout in seconds when establishing the WebSocket session
TRANSLATION_CONNECT_TIMEOUT=10.0

# =============================================================================
# Framework Configuration
# =============================================================================

# Directory where test artifacts (audio, transcripts, logs) will be stored
TRANSLATION_RESULTS_DIR=production_results

# Time acceleration factor for scenario playback (1.0 = real time, 2.0 = 2x speed)
TRANSLATION_TIME_ACCELERATION=1.0

# Milliseconds of trailing silence to stream after the final event
# Allows downstream translations to finish before closing the connection
TRANSLATION_TAIL_SILENCE_MS=10000

# Whether to log all inbound/outbound WebSocket messages (true/false)
TRANSLATION_DEBUG_WIRE=false

# =============================================================================
# Metrics Storage (Optional)
# =============================================================================

# Enable MongoDB storage for test metrics
# Set to "true" to persist metrics, "false" to run without storage
MONGODB_ENABLED=false

# MongoDB connection string
# Local: mongodb://localhost:27017
# Atlas: mongodb+srv://username:password@cluster.mongodb.net/?retryWrites=true&w=majority
MONGODB_CONNECTION_STRING=mongodb://localhost:27017

# MongoDB database name
MONGODB_DATABASE=vt_metrics

# Environment classification for evaluation runs
# Options: dev, stage, prod, lab
ENVIRONMENT=dev

# Experiment tags (comma-separated)
# Use to label evaluation runs for comparison and filtering
# Examples: "baseline", "config-tweak", "latency-optimization"
EXPERIMENT_TAGS=

# =============================================================================
# LLM Service (for Metrics Evaluation)
# =============================================================================

# Azure AI Foundry API key for LLM-based metrics
# Required for: TechnicalTermsMetric, CompletenessMetric, IntentPreservationMetric, LanguageCorrectnessMetric
AZURE_AI_FOUNDRY_KEY=your_azure_ai_foundry_key_here

# OpenAI API base URL (Azure OpenAI endpoint)
# Example: https://your-resource.openai.azure.com/
OPENAI_BASE_URL=https://your-resource.openai.azure.com/

# LLM model to use for evaluation
# Default: gpt-4o-mini (cost-effective)
# Options: gpt-4o-mini, gpt-4o, gpt-4, gpt-3.5-turbo
LLM_MODEL=gpt-4o-mini

# =============================================================================
# Remote Debugging (Optional - PyCharm/IntelliJ)
# =============================================================================

# Enable PyCharm remote debugging (true/false)
TRANSLATION_REMOTE_DEBUG=false

# Debug server host (where PyCharm/IntelliJ is running)
TRANSLATION_DEBUG_HOST=localhost

# Debug server port (must match PyCharm's "Python Debug Server" configuration)
TRANSLATION_DEBUG_PORT=5678

# Wait for debugger to attach before continuing execution (true/false)
TRANSLATION_DEBUG_SUSPEND=false

# Redirect stdout to the debugger console (true/false)
TRANSLATION_DEBUG_STDOUT=true

# Redirect stderr to the debugger console (true/false)
TRANSLATION_DEBUG_STDERR=true
