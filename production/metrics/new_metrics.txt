--------------
MAIN TICKET
--------------
Description
As a product team,
I want an automated way to score conversational quality (per turn and per session),
so we can detect when transcripts are garbled or context-breaking and track UX health over time.

Acceptance Criteria

Each bot turn can be scored with:

intelligibility_score (1–5)

segmentation_score (1–5)

context_score (1–5)

garbled (bool, derived from scores)

At least one known "context loss" example (UTI question) is flagged with context_score ≤ 2 and garbled = true.

Hallucination/drift example (paddle → soccer) gets high intelligibility/segmentation (≥ 4) but mid/low context (2–3), not 1.

Clean "good" conversations (to be provided) have:

garbled_turn_rate below a configurable threshold (e.g., < 10%)

Average intelligibility, segmentation, context ≥ 4

Metrics are persisted per session/turn in a queryable form (DB/logs) and can be pulled into a basic report or dashboard.

--------------
TASK 1
--------------
Description
Define the per-turn scoring rubric and JSON schema, plus concise prompts for the evaluator LLM to return intelligibility_score, segmentation_score, context_score, garbled, and notes.

Acceptance Criteria

JSON schema for the per-turn result is documented (field names, types, ranges).

At least one prompt template exists for:

Single-utterance scoring with optional prior context.

Running the prompt manually on a few provided examples yields scores that match our expectations (e.g., UTI example has low context).

--------------
TASK 2
--------------

Description
Implement a Python module/CLI that, given conversation transcripts, calls the evaluator LLM and outputs per-turn scores and session aggregates.

Acceptance Criteria

Given a JSON/NDJSON conversation file, the tool outputs:

Per-turn scores (intelligibility_score, segmentation_score, context_score, garbled, notes)

Session-level aggregates (avg_*, garbled_turn_rate)

The UTI context-loss example is flagged as garbled = true.

Paddle → soccer example shows good intelligibility/segmentation but degraded context.

Clean sample conversations produce high averages and low garbled_turn_rate.

Basic error handling is in place (retry on bad JSON, timeouts, etc.).